{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Attribution for *Dream of the Red Chamber* (红楼梦)\n",
    "\n",
    "- author: 'chaunice@163.com'\n",
    "- date: '2024-01-26'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "with reference to [Prose, Verse and Authorship in Dream of the Red Chamber: A Stylometric Analysis](https://www.researchgate.net/profile/Haoran-Zhu-8/publication/339140299_Prose_Verse_and_Authorship_in_Dream_of_the_Red_Chamber_A_Stylometric_Analysis/links/650fccd661f18040c21a3ff9/Prose-Verse-and-Authorship-in-Dream-of-the-Red-Chamber-A-Stylometric-Analysis.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "- DRC text from https://github.com/buobao/HLM-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from statsmodels.stats.weightstats import ttest_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_verses_and_prose(data):\n",
    "    # Compile regex patterns\n",
    "    verse_pattern = re.compile(r\"<blockquote([\\s\\S]*?)</blockquote>\")\n",
    "    prose_pattern = re.compile(r\"<p>([\\s\\S]*?)</p>\")\n",
    "\n",
    "    # Find all verses\n",
    "    verses = re.findall(verse_pattern, data)\n",
    "    verses = [verse.strip() for verse in verses]  # Remove leading/trailing whitespace\n",
    "\n",
    "    # Remove verses from data\n",
    "    data = re.sub(verse_pattern, \"\", data)\n",
    "\n",
    "    # Find all prose\n",
    "    prose = re.findall(prose_pattern, data)\n",
    "    prose = [p.strip() for p in prose]  # Remove leading/trailing whitespace\n",
    "\n",
    "    return verses, prose\n",
    "\n",
    "\n",
    "def glob_markdown_files(directory):\n",
    "    return list(Path(directory).glob(\"*.md\"))\n",
    "\n",
    "\n",
    "# batch process all files and store results in a csv while keeping track of the file name\n",
    "def batch_process_files(directory):\n",
    "    markdown_files = glob_markdown_files(directory)\n",
    "    results = []\n",
    "    for file in markdown_files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = f.read()\n",
    "        verses, prose = separate_verses_and_prose(data)\n",
    "        results.append({\"File\": file.name, \"Verses\": verses, \"Prose\": prose})\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"DRC_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_process_files(\"full/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DRC_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the text by removing punctuations and only retaining chinese characters\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"Verses\"] = df[\"Verses\"].apply(lambda x: [clean_text(verse) for verse in x])\n",
    "df[\"Prose\"] = df[\"Prose\"].apply(lambda x: [clean_text(p) for p in x])\n",
    "\n",
    "# remove empty verses and prose\n",
    "df[\"Verses\"] = df[\"Verses\"].apply(lambda x: [verse for verse in x if verse])\n",
    "df[\"Prose\"] = df[\"Prose\"].apply(lambda x: [p for p in x if p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"DRC_text_clean.csv\", index=False, sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DRC_text_clean.csv\", sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of characters in each verse and prose group by 10 files so that we get 12 groups\n",
    "df[\"VerseLength\"] = df[\"Verses\"].apply(lambda x: [len(verse) for verse in x])\n",
    "df[\"ProseLength\"] = df[\"Prose\"].apply(lambda x: [len(p) for p in x])\n",
    "df[\"VerseLength\"] = df[\"VerseLength\"].apply(lambda x: [len(x)])\n",
    "df[\"ProseLength\"] = df[\"ProseLength\"].apply(lambda x: [len(x)])\n",
    "\n",
    "# 10 files as a group\n",
    "df[\"Groups\"] = (df.index // 10) + 1\n",
    "\n",
    "# sum the length of verses and prose in each group\n",
    "df = df.groupby(\"Groups\").sum()\n",
    "# calculate the sum of verses and prose in each group\n",
    "df[\"VerseCount\"] = df[\"VerseLength\"].apply(lambda x: sum(x))\n",
    "df[\"ProseCount\"] = df[\"ProseLength\"].apply(lambda x: sum(x))\n",
    "\n",
    "df[\"Proportion of Verse\"] = df[\"VerseCount\"] / (df[\"VerseCount\"] + df[\"ProseCount\"])\n",
    "\n",
    "df[\"Groups\"] = df.index\n",
    "\n",
    "# 用正则表达式删除所有标点符号和空格\n",
    "df[\"Verses\"] = df[\"Verses\"].apply(lambda x: re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", x))\n",
    "df[\"Prose\"] = df[\"Prose\"].apply(lambda x: re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", x))\n",
    "\n",
    "# 创建一个新的列来区分前4组，中间4组和后4组\n",
    "df[\"Group_sep\"] = [\"Group 1-4\"] * 4 + [\"Group 5-8\"] * 4 + [\"Group 9-12\"] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results concentrating 'Proportion of Verse'\n",
    "# 创建一个字典，将组名映射到 Chicago 配色的颜色代码\n",
    "chicago_palette = {\n",
    "    \"Group 1-4\": \"#0b6623\",\n",
    "    \"Group 5-8\": \"#0F425CFF\",\n",
    "    \"Group 9-12\": \"#a40e4c\",\n",
    "}\n",
    "\n",
    "# 创建柱状图\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(\n",
    "    x=df.index,\n",
    "    y=df[\"Proportion of Verse\"],\n",
    "    hue=df[\"Group_sep\"],\n",
    "    dodge=False,\n",
    "    palette=chicago_palette,\n",
    ")\n",
    "\n",
    "# 计算前8组和后4组的平均值\n",
    "average_line_1_4 = df.loc[1:4, \"Proportion of Verse\"].mean()\n",
    "average_line_5_8 = df.loc[5:8, \"Proportion of Verse\"].mean()\n",
    "average_line_9_12 = df.loc[9:12, \"Proportion of Verse\"].mean()\n",
    "\n",
    "# 绘制平均线\n",
    "plt.axhline(\n",
    "    average_line_1_4, color=\"#0b6623\", linestyle=\"--\", label=\"Average Line (Group 1-8)\"\n",
    ")\n",
    "plt.axhline(\n",
    "    average_line_5_8,\n",
    "    color=\"#0F425CFF\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Average Line (Group 5-8)\",\n",
    ")\n",
    "plt.axhline(\n",
    "    average_line_9_12,\n",
    "    color=\"#a40e4c\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Average Line (Group 9-12)\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Groups\")\n",
    "plt.ylabel(\"Proportion of Verse\")\n",
    "plt.title(\"Proportion of Verse in Each Group\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform T-test to see if there is a significant difference between group 1-4, group 5-8 and group 9-12\n",
    "ttest_df = df[[\"Proportion of Verse\", \"Group_sep\"]]\n",
    "\n",
    "# 将数据分成三组\n",
    "group_1_4 = ttest_df[ttest_df[\"Group_sep\"] == \"Group 1-4\"][\"Proportion of Verse\"]\n",
    "group_5_8 = ttest_df[ttest_df[\"Group_sep\"] == \"Group 5-8\"][\"Proportion of Verse\"]\n",
    "group_9_12 = ttest_df[ttest_df[\"Group_sep\"] == \"Group 9-12\"][\"Proportion of Verse\"]\n",
    "\n",
    "# 进行 T 检验\n",
    "t1, p1, df1 = ttest_ind(group_1_4, group_5_8)\n",
    "t2, p2, df2 = ttest_ind(group_1_4, group_9_12)\n",
    "t3, p3, df3 = ttest_ind(group_5_8, group_9_12)\n",
    "\n",
    "print(\"T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f\" % (t1, p1))\n",
    "print(\"T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f\" % (t2, p2))\n",
    "print(\"T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f\" % (t3, p3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 Prose 进行取样，去尾取样， 分成 segments，每个 segment 有 segment_size 个字符\n",
    "def sample_prose(prose, segment_size):\n",
    "    # 去尾取样\n",
    "    remainder = len(prose) % segment_size\n",
    "    prose = prose[:-remainder]\n",
    "\n",
    "    # 分成连续的 segments，每个 segment 有 segment_size 个字符\n",
    "    segments = [prose[i : i + segment_size] for i in range(0, len(prose), segment_size)]\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，找到合并所有的 Segmented Prose 后最频繁出现的 n 个字符\n",
    "def get_top_n_chars(df, n):\n",
    "    # 合并所有的 Segmented Prose\n",
    "    prose = \"\".join(df[\"Segmented Prose\"].tolist())\n",
    "\n",
    "    # 使用 Counter 来统计每个字符出现的次数\n",
    "    char_counts = Counter(prose)\n",
    "\n",
    "    # 将字符按出现次数从高到低排序\n",
    "    char_counts = char_counts.most_common(n)\n",
    "\n",
    "    # 返回前 n 个字符\n",
    "    return [char[0] for char in char_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca(df, segment_size, n, original=False):\n",
    "    df = df.copy()\n",
    "\n",
    "    # df 保留 Prose, 然后按 Group_sep 合并\n",
    "    df = df[[\"Prose\", \"Group_sep\"]]\n",
    "    df = df.groupby(\"Group_sep\").sum()\n",
    "\n",
    "    # 对 prose 进行取样\n",
    "    df[\"Segmented Prose\"] = df[\"Prose\"].apply(lambda x: sample_prose(x, segment_size))\n",
    "\n",
    "    if original:\n",
    "        # 合并 Group_1-4 和 Group_5-8\n",
    "        df.loc[\"Group 1-8\"] = df.loc[\"Group 1-4\"] + df.loc[\"Group 5-8\"]\n",
    "        # 删除 Group_1-4 和 Group_5-8\n",
    "        df.drop([\"Group 1-4\", \"Group 5-8\"], inplace=True)\n",
    "\n",
    "        # 展开列表\n",
    "        seg_df = df.explode(\"Segmented Prose\").reset_index()\n",
    "        seg_df = seg_df.drop(\"Prose\", axis=1)\n",
    "\n",
    "        # 找到最频繁出现的 n 个字符\n",
    "        mfc_list = get_top_n_chars(seg_df, n)\n",
    "\n",
    "        char_counts = {\n",
    "            char: seg_df[\"Segmented Prose\"].apply(lambda x: x.count(char))\n",
    "            for char in mfc_list\n",
    "        }\n",
    "\n",
    "        # 将结果添加到 seg_df 中\n",
    "        seg_df = pd.concat([seg_df, pd.DataFrame(char_counts)], axis=1)\n",
    "\n",
    "        seg_df.drop(\"Segmented Prose\", axis=1, inplace=True)\n",
    "\n",
    "        sample_num = seg_df.query('Group_sep == \"Group 9-12\"').shape[0]\n",
    "\n",
    "        # 对 Group 1-8 进行下采样，保证 Group 1-8 和 Group 9-12 的数量一致，并且随机种子为 42\n",
    "        seg_df = (\n",
    "            seg_df.groupby(\"Group_sep\")\n",
    "            .apply(lambda x: x.sample(n=sample_num, random_state=42))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # 展开列表\n",
    "        seg_df = df.explode(\"Segmented Prose\").reset_index()\n",
    "        seg_df = seg_df.drop(\"Prose\", axis=1)\n",
    "\n",
    "        # 找到最频繁出现的 n 个字符\n",
    "        mfc_list = get_top_n_chars(seg_df, n)\n",
    "\n",
    "        char_counts = {\n",
    "            char: seg_df[\"Segmented Prose\"].apply(lambda x: x.count(char))\n",
    "            for char in mfc_list\n",
    "        }\n",
    "\n",
    "        # 将结果添加到 seg_df 中\n",
    "        seg_df = pd.concat([seg_df, pd.DataFrame(char_counts)], axis=1)\n",
    "\n",
    "        seg_df.drop(\"Segmented Prose\", axis=1, inplace=True)\n",
    "\n",
    "    # 进行主成分分析\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(seg_df.drop(\"Group_sep\", axis=1))\n",
    "    pca_df = pd.DataFrame(\n",
    "        pca.transform(seg_df.drop(\"Group_sep\", axis=1)), columns=[\"PC1\", \"PC2\"]\n",
    "    )\n",
    "    pca_df[\"Group_sep\"] = seg_df[\"Group_sep\"]\n",
    "\n",
    "    # return Segmented_Prose_list_1_8, Segmented_Prose_list_9_12, df2, seg_df\n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_results(df, original=False):\n",
    "    # 初始化一个空字典来保存结果\n",
    "    pca_results = {}\n",
    "\n",
    "    # 遍历样本大小和 n 的组合\n",
    "    for segment_size in [1000, 2000, 3000, 4000, 5000]:\n",
    "        for n in [50, 75, 100, 125, 150]:\n",
    "            # 执行 PCA\n",
    "            pca_df = perform_pca(df, segment_size, n, original=original)\n",
    "\n",
    "            # 保存结果\n",
    "            pca_results[(segment_size, n)] = pca_df\n",
    "\n",
    "    return pca_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_results(pca_results, original=False):\n",
    "    # 创建一个 5x5 的图像网格\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(25, 25))\n",
    "\n",
    "    # 遍历所有的结果\n",
    "    for i, ((sample_size, max_features), temp_df_pca) in enumerate(pca_results.items()):\n",
    "        # 计算当前子图的位置\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "\n",
    "        # 绘制散点图\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        chicago_palette = {\n",
    "            \"Group 1-4\": \"#C16622FF\",\n",
    "            \"Group 5-8\": \"#155F83FF\",\n",
    "            \"Group 9-12\": \"#8A9045FF\",\n",
    "        }\n",
    "        # chicago_palette = {'Group 1-4': '#C16622FF', 'Group 5-8': '#155F83FF', 'Group 9-12': '#FFFFFF00'}\n",
    "        # chicago_palette = {'Group 1-4': '#C16622FF', 'Group 5-8': '#FFFFFF00', 'Group 9-12': '#155F83FF'}\n",
    "        # chicago_palette = {'Group 1-4': '#FFFFFF00', 'Group 5-8': '#C16622FF', 'Group 9-12': '#155F83FF'}\n",
    "\n",
    "        if original:\n",
    "            chicago_palette = {\"Group 1-8\": \"#C16622FF\", \"Group 9-12\": \"#155F83FF\"}\n",
    "\n",
    "        sns.scatterplot(\n",
    "            x=\"PC1\",\n",
    "            y=\"PC2\",\n",
    "            hue=\"Group_sep\",\n",
    "            data=temp_df_pca,\n",
    "            palette=chicago_palette,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # 设置标题\n",
    "        ax.set_title(f\"Sample Size: {sample_size}, Max Features: {max_features}\")\n",
    "\n",
    "    # 调整子图之间的间距\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_conducting_PCA(df, original=False):\n",
    "    plot_pca_results(get_pca_results(df, original=original), original=original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_conducting_PCA(df, original=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE: t-distributed Stochastic Neighbor Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tsne(df, segment_size, n, original=False):\n",
    "    # df 保留 Prose, 然后按 Group_sep 合并\n",
    "    df = df[[\"Prose\", \"Group_sep\"]]\n",
    "    df = df.groupby(\"Group_sep\").sum()\n",
    "\n",
    "    # 对 prose 进行取样\n",
    "    df[\"Segmented Prose\"] = df[\"Prose\"].apply(lambda x: sample_prose(x, segment_size))\n",
    "\n",
    "    if original:\n",
    "        # 合并 Group_1-4 和 Group_5-8\n",
    "        df.loc[\"Group 1-8\"] = df.loc[\"Group 1-4\"] + df.loc[\"Group 5-8\"]\n",
    "        # 删除 Group_1-4 和 Group_5-8\n",
    "        df.drop([\"Group 1-4\", \"Group 5-8\"], inplace=True)\n",
    "\n",
    "        # 展开列表\n",
    "        seg_df = df.explode(\"Segmented Prose\").reset_index()\n",
    "        seg_df = seg_df.drop(\"Prose\", axis=1)\n",
    "\n",
    "        # 找到最频繁出现的 n 个字符\n",
    "        mfc_list = get_top_n_chars(seg_df, n)\n",
    "\n",
    "        char_counts = {\n",
    "            char: seg_df[\"Segmented Prose\"].apply(lambda x: x.count(char))\n",
    "            for char in mfc_list\n",
    "        }\n",
    "\n",
    "        # 将结果添加到 seg_df 中\n",
    "        seg_df = pd.concat([seg_df, pd.DataFrame(char_counts)], axis=1)\n",
    "\n",
    "        seg_df.drop(\"Segmented Prose\", axis=1, inplace=True)\n",
    "\n",
    "        sample_num = seg_df.query('Group_sep == \"Group 9-12\"').shape[0]\n",
    "\n",
    "        # 对 Group 1-8 进行下采样，保证 Group 1-8 和 Group 9-12 的数量一致，并且随机种子为 42\n",
    "        seg_df = (\n",
    "            seg_df.groupby(\"Group_sep\")\n",
    "            .apply(lambda x: x.sample(n=sample_num, random_state=42))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # 展开列表\n",
    "        seg_df = df.explode(\"Segmented Prose\").reset_index()\n",
    "        seg_df = seg_df.drop(\"Prose\", axis=1)\n",
    "\n",
    "        # 找到最频繁出现的 n 个字符\n",
    "        mfc_list = get_top_n_chars(seg_df, n)\n",
    "\n",
    "        char_counts = {\n",
    "            char: seg_df[\"Segmented Prose\"].apply(lambda x: x.count(char))\n",
    "            for char in mfc_list\n",
    "        }\n",
    "\n",
    "        # 将结果添加到 seg_df 中\n",
    "        seg_df = pd.concat([seg_df, pd.DataFrame(char_counts)], axis=1)\n",
    "\n",
    "        seg_df.drop(\"Segmented Prose\", axis=1, inplace=True)\n",
    "\n",
    "    # 进行 t-SNE\n",
    "    tsne = TSNE(n_components=2)\n",
    "    tsne.fit(seg_df.drop(\"Group_sep\", axis=1))\n",
    "    tsne_df = pd.DataFrame(tsne.embedding_, columns=[\"tsne1\", \"tsne2\"])\n",
    "    tsne_df[\"Group_sep\"] = seg_df[\"Group_sep\"]\n",
    "\n",
    "    return tsne_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tsne_results(df, original=False):\n",
    "    # 初始化一个空字典来保存结果\n",
    "    tsne_results = {}\n",
    "\n",
    "    # 遍历样本大小和 n 的组合\n",
    "    for segment_size in [1000, 2000, 3000, 4000, 5000]:\n",
    "        for n in [50, 75, 100, 125, 150]:\n",
    "            # 执行 t-SNE\n",
    "            tsne_df = perform_tsne(df, segment_size, n, original=original)\n",
    "\n",
    "            # 保存结果\n",
    "            tsne_results[(segment_size, n)] = tsne_df\n",
    "\n",
    "    return tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_results(tsne_results, original=False):\n",
    "    # 创建一个 5x5 的图像网格\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(25, 25))\n",
    "\n",
    "    # 遍历所有的结果\n",
    "    for i, ((sample_size, max_features), temp_df_tsne) in enumerate(\n",
    "        tsne_results.items()\n",
    "    ):\n",
    "        # 计算当前子图的位置\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "\n",
    "        # 绘制散点图\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        chicago_palette = {\n",
    "            \"Group 1-4\": \"#C16622FF\",\n",
    "            \"Group 5-8\": \"#155F83FF\",\n",
    "            \"Group 9-12\": \"#8A9045FF\",\n",
    "        }\n",
    "        # chicago_palette = {'Group 1-4': '#C16622FF', 'Group 5-8': '#155F83FF', 'Group 9-12': '#FFFFFF00'}\n",
    "        # chicago_palette = {'Group 1-4': '#C16622FF', 'Group 5-8': '#FFFFFF00', 'Group 9-12': '#155F83FF'}\n",
    "        # chicago_palette = {'Group 1-4': '#FFFFFF00', 'Group 5-8': '#C16622FF', 'Group 9-12': '#155F83FF'}\n",
    "\n",
    "        if original:\n",
    "            chicago_palette = {\"Group 1-8\": \"#C16622FF\", \"Group 9-12\": \"#155F83FF\"}\n",
    "\n",
    "        sns.scatterplot(\n",
    "            x=\"tsne1\",\n",
    "            y=\"tsne2\",\n",
    "            hue=\"Group_sep\",\n",
    "            data=temp_df_tsne,\n",
    "            palette=chicago_palette,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # 设置标题\n",
    "        ax.set_title(f\"Sample Size: {sample_size}, Max Features: {max_features}\")\n",
    "\n",
    "    # 调整子图之间的间距\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_conducting_tsne(df, original=False):\n",
    "    plot_tsne_results(get_tsne_results(df, original=original), original=original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_conducting_tsne(df, original=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA: Principal Component Analysis [Chaunice's version]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，从一个字符串中随机抽取n个字符\n",
    "def sample_chars(s, n):\n",
    "    return \"\".join(random.sample(s, min(n, len(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmex_pca(dataframe, sample_size, max_features, n_components):\n",
    "    # 创建 df 的副本\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # 重复100次，每次都从每个组中随机抽取 sample_size 个字符，然后计算字符计数矩阵\n",
    "    char_counts_list = []\n",
    "    for i in range(100):\n",
    "        df_sample = (\n",
    "            df.groupby(\"Group_sep\")\n",
    "            .apply(\n",
    "                lambda x: sample_chars(\"\".join(x[\"Prose\"]), sample_size),\n",
    "                include_groups=False,\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        df_sample[0] = df_sample[0].apply(lambda x: \"\".join(list(x)))\n",
    "        vectorizer = CountVectorizer(analyzer=\"char\", max_features=max_features)\n",
    "        char_counts = vectorizer.fit_transform(df_sample[0])\n",
    "        char_counts_df = pd.DataFrame(\n",
    "            char_counts.toarray(), columns=vectorizer.get_feature_names_out()\n",
    "        )\n",
    "        char_counts_list.append(char_counts_df)\n",
    "\n",
    "    # 将所有字符计数矩阵连接起来，保留分组信息\n",
    "    char_counts_df = pd.concat(char_counts_list, keys=range(100))\n",
    "\n",
    "    # 将空值替换为 0\n",
    "    char_counts_df = char_counts_df.fillna(0)\n",
    "\n",
    "    # 转换 char_counts_df，将分组信息转换为列\n",
    "    char_counts_df = char_counts_df.reset_index().rename(\n",
    "        columns={\"level_0\": \"Sample\", \"level_1\": \"Group_sep\"}\n",
    "    )\n",
    "\n",
    "    # 分组进行PCA，保留分组信息方便后续绘图\n",
    "    pca = PCA(n_components=n_components)\n",
    "    char_counts_df_pca = pca.fit_transform(\n",
    "        char_counts_df.drop([\"Sample\", \"Group_sep\"], axis=1)\n",
    "    )\n",
    "    char_counts_df_pca = pd.DataFrame(char_counts_df_pca, columns=[\"PC1\", \"PC2\"])\n",
    "    char_counts_df_pca[\"Sample\"] = char_counts_df[\"Sample\"]\n",
    "    char_counts_df_pca[\"Group_sep\"] = char_counts_df[\"Group_sep\"]\n",
    "    return char_counts_df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个空字典来保存结果\n",
    "cmex_pca_results = {}\n",
    "\n",
    "# 遍历样本大小\n",
    "for sample_size in [1000, 2000, 3000, 4000, 5000]:\n",
    "    # 遍历特征数量\n",
    "    for max_features in [50, 75, 100, 125, 150]:\n",
    "        # 调用函数并保存结果\n",
    "        key = (sample_size, max_features)\n",
    "        cmex_pca_results[key] = cmex_pca(df, sample_size, max_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 5x5 的图像网格\n",
    "fig, axs = plt.subplots(5, 5, figsize=(25, 25))\n",
    "\n",
    "# 遍历所有的结果\n",
    "for i, ((sample_size, max_features), cmex_temp_df_pca) in enumerate(\n",
    "    cmex_pca_results.items()\n",
    "):\n",
    "    # 计算当前子图的位置\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "\n",
    "    # 在当前子图中绘制散点图\n",
    "    ax = axs[row, col]\n",
    "    chicago_palette = {0: \"#C16622FF\", 1: \"#155F83FF\", 2: \"#8A9045FF\"}\n",
    "    sns.scatterplot(\n",
    "        data=cmex_temp_df_pca,\n",
    "        x=\"PC1\",\n",
    "        y=\"PC2\",\n",
    "        hue=\"Group_sep\",\n",
    "        palette=chicago_palette,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # 设置子图的标题\n",
    "    ax.set_title(f\"Sample size: {sample_size}, Max features: {max_features}\")\n",
    "\n",
    "# 调整子图之间的间距\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test on PCA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ((sample_size, max_features), temp_df_pca_ttest) in enumerate(\n",
    "    cmex_pca_results.items()\n",
    "):\n",
    "    pca_result_ttest_df = temp_df_pca_ttest[[\"PC1\", \"PC2\", \"Group_sep\"]]\n",
    "\n",
    "    # transform the dataframe to a long format\n",
    "\n",
    "    pca_result_ttest_df = pca_result_ttest_df.melt(\n",
    "        id_vars=[\"Group_sep\"], value_vars=[\"PC1\", \"PC2\"]\n",
    "    )\n",
    "\n",
    "    # retain Group_sep, meanwhile transform PC1 and PC2 to separate columns, turn the values to a list\n",
    "\n",
    "    pca_result_ttest_df = (\n",
    "        pca_result_ttest_df.groupby([\"Group_sep\", \"variable\"])[\"value\"]\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # perform T-test to see if there is a significant difference between group 1-4, group 5-8 and group 9-12\n",
    "\n",
    "    ttest_df = pca_result_ttest_df[[\"value\", \"Group_sep\", \"variable\"]]\n",
    "\n",
    "    ttest_df = ttest_df.explode(\"value\")\n",
    "\n",
    "    # 将数据分成三组\n",
    "\n",
    "    group_1_4_PC1 = ttest_df.query('variable == \"PC1\" and Group_sep == 0')[\"value\"]\n",
    "\n",
    "    group_5_8_PC1 = ttest_df.query('variable == \"PC1\" and Group_sep == 1')[\"value\"]\n",
    "\n",
    "    group_9_12_PC1 = ttest_df.query('variable == \"PC1\" and Group_sep == 2')[\"value\"]\n",
    "\n",
    "    group_1_4_PC2 = ttest_df.query('variable == \"PC2\" and Group_sep == 0')[\"value\"]\n",
    "\n",
    "    group_5_8_PC2 = ttest_df.query('variable == \"PC2\" and Group_sep == 1')[\"value\"]\n",
    "\n",
    "    group_9_12_PC2 = ttest_df.query('variable == \"PC2\" and Group_sep == 2')[\"value\"]\n",
    "\n",
    "    # 进行 T 检验\n",
    "\n",
    "    t1_PC1, p1_PC1, df1_PC1 = ttest_ind(group_1_4_PC1, group_5_8_PC1)\n",
    "\n",
    "    t2_PC1, p2_PC1, df2_PC1 = ttest_ind(group_1_4_PC1, group_9_12_PC1)\n",
    "\n",
    "    t3_PC1, p3_PC1, df3_PC1 = ttest_ind(group_5_8_PC1, group_9_12_PC1)\n",
    "\n",
    "    t1_PC2, p1_PC2, df1_PC2 = ttest_ind(group_1_4_PC2, group_5_8_PC2)\n",
    "\n",
    "    t2_PC2, p2_PC2, df2_PC2 = ttest_ind(group_1_4_PC2, group_9_12_PC2)\n",
    "\n",
    "    t3_PC2, p3_PC2, df3_PC2 = ttest_ind(group_5_8_PC2, group_9_12_PC2)\n",
    "\n",
    "    print(f\"Sample size: {sample_size}, Max features: {max_features}\")\n",
    "\n",
    "    print(\"T-test results for PC1:\")\n",
    "\n",
    "    print(\n",
    "        \"T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f\"\n",
    "        % (t1_PC1, p1_PC1)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f\"\n",
    "        % (t2_PC1, p2_PC1)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f\"\n",
    "        % (t3_PC1, p3_PC1)\n",
    "    )\n",
    "\n",
    "    print(\"T-test results for PC2:\")\n",
    "\n",
    "    print(\n",
    "        \"T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f\"\n",
    "        % (t1_PC2, p1_PC2)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f\"\n",
    "        % (t2_PC2, p2_PC2)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f\"\n",
    "        % (t3_PC2, p3_PC2)\n",
    "    )\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE: t-distributed Stochastic Neighbor Embedding [Chaunice's version]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform t-sne to see if there is a significant difference between group 1-4, group 5-8 and group 9-12\n",
    "def cmex_tsne(dataframe, sample_size, max_features, n_components):\n",
    "    # 创建 df 的副本\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # 重复100次，每次都从每个组中随机抽取 max_features 个字符，然后计算字符计数矩阵\n",
    "    char_counts_list = []\n",
    "    for i in range(100):\n",
    "        df_sample = (\n",
    "            df.groupby(\"Group_sep\")\n",
    "            .apply(\n",
    "                lambda x: sample_chars(\"\".join(x[\"Prose\"]), sample_size),\n",
    "                include_groups=False,\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        df_sample[0] = df_sample[0].apply(lambda x: \"\".join(list(x)))\n",
    "        vectorizer = CountVectorizer(analyzer=\"char\", max_features=max_features)\n",
    "        char_counts = vectorizer.fit_transform(df_sample[0])\n",
    "        char_counts_df = pd.DataFrame(\n",
    "            char_counts.toarray(), columns=vectorizer.get_feature_names_out()\n",
    "        )\n",
    "        char_counts_list.append(char_counts_df)\n",
    "\n",
    "    # 将所有字符计数矩阵连接起来，保留分组信息\n",
    "    char_counts_df = pd.concat(char_counts_list, keys=range(100))\n",
    "\n",
    "    # 将空值替换为 0\n",
    "    char_counts_df = char_counts_df.fillna(0)\n",
    "\n",
    "    # 转换 char_counts_df，将分组信息转换为列\n",
    "    char_counts_df = char_counts_df.reset_index().rename(\n",
    "        columns={\"level_0\": \"Sample\", \"level_1\": \"Group_sep\"}\n",
    "    )\n",
    "\n",
    "    # 分组进行t-sne，保留分组信息方便后续绘图\n",
    "    tsne = TSNE(n_components=n_components)\n",
    "    char_counts_df_tsne = tsne.fit_transform(\n",
    "        char_counts_df.drop([\"Sample\", \"Group_sep\"], axis=1)\n",
    "    )\n",
    "    char_counts_df_tsne = pd.DataFrame(char_counts_df_tsne, columns=[\"tsne1\", \"tsne2\"])\n",
    "    char_counts_df_tsne[\"Sample\"] = char_counts_df[\"Sample\"]\n",
    "    char_counts_df_tsne[\"Group_sep\"] = char_counts_df[\"Group_sep\"]\n",
    "    return char_counts_df_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个空字典来保存结果\n",
    "cmex_tsne_results = {}\n",
    "\n",
    "# 遍历样本大小\n",
    "for sample_size in [1000, 2000, 3000, 4000, 5000]:\n",
    "    # 遍历特征数量\n",
    "    for max_features in [50, 75, 100, 125, 150]:\n",
    "        # 调用函数并保存结果\n",
    "        key = (sample_size, max_features)\n",
    "        cmex_tsne_results[key] = cmex_tsne(df, sample_size, max_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 5x5 的图像网格\n",
    "fig, axs = plt.subplots(5, 5, figsize=(25, 25))\n",
    "\n",
    "# 遍历所有的结果\n",
    "for i, ((sample_size, max_features), cmex_temp_df_tsne) in enumerate(\n",
    "    cmex_tsne_results.items()\n",
    "):\n",
    "    # 计算当前子图的位置\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "\n",
    "    # 在当前子图中绘制散点图\n",
    "    ax = axs[row, col]\n",
    "    chicago_palette = {0: \"#C16622FF\", 1: \"#155F83FF\", 2: \"#8A9045FF\"}\n",
    "    sns.scatterplot(\n",
    "        data=cmex_temp_df_tsne,\n",
    "        x=\"tsne1\",\n",
    "        y=\"tsne2\",\n",
    "        hue=\"Group_sep\",\n",
    "        palette=chicago_palette,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # 设置子图的标题\n",
    "    ax.set_title(f\"Sample size: {sample_size}, Max features: {max_features}\")\n",
    "\n",
    "# 调整子图之间的间距\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test on T-SNE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ((sample_size, max_features), temp_df_tsne_ttest) in enumerate(\n",
    "    cmex_tsne_results.items()\n",
    "):\n",
    "    tsne_result_ttest_df = temp_df_tsne_ttest[[\"tsne1\", \"tsne2\", \"Group_sep\"]]\n",
    "\n",
    "    # transform the dataframe to a long format\n",
    "    tsne_result_ttest_df = tsne_result_ttest_df.melt(\n",
    "        id_vars=[\"Group_sep\"], value_vars=[\"tsne1\", \"tsne2\"]\n",
    "    )\n",
    "\n",
    "    # retain Group_sep, meanwhile transform PC1 and PC2 to separate columns, turn the values to a list\n",
    "    tsne_result_ttest_df = (\n",
    "        tsne_result_ttest_df.groupby([\"Group_sep\", \"variable\"])[\"value\"]\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # perform T-test to see if there is a significant difference between group 1-4, group 5-8 and group 9-12\n",
    "    ttest_df = tsne_result_ttest_df[[\"value\", \"Group_sep\", \"variable\"]]\n",
    "    ttest_df = ttest_df.explode(\"value\")\n",
    "\n",
    "    # 将数据分成三组\n",
    "    group_1_4_tsne1 = ttest_df.query('variable == \"tsne1\" and Group_sep == 0')[\"value\"]\n",
    "    group_5_8_tsne1 = ttest_df.query('variable == \"tsne1\" and Group_sep == 1')[\"value\"]\n",
    "    group_9_12_tsne1 = ttest_df.query('variable == \"tsne1\" and Group_sep == 2')[\"value\"]\n",
    "\n",
    "    group_1_4_tsne2 = ttest_df.query('variable == \"tsne2\" and Group_sep == 0')[\"value\"]\n",
    "    group_5_8_tsne2 = ttest_df.query('variable == \"tsne2\" and Group_sep == 1')[\"value\"]\n",
    "    group_9_12_tsne2 = ttest_df.query('variable == \"tsne2\" and Group_sep == 2')[\"value\"]\n",
    "\n",
    "    # 进行 T 检验\n",
    "    t1_tsne1, p1_tsne1, df1_tsne1 = ttest_ind(group_1_4_tsne1, group_5_8_tsne1)\n",
    "    t2_tsne1, p2_tsne1, df2_tsne1 = ttest_ind(group_1_4_tsne1, group_9_12_tsne1)\n",
    "    t3_tsne1, p3_tsne1, df3_tsne1 = ttest_ind(group_5_8_tsne1, group_9_12_tsne1)\n",
    "\n",
    "    t1_tsne2, p1_tsne2, df1_tsne2 = ttest_ind(group_1_4_tsne2, group_5_8_tsne2)\n",
    "    t2_tsne2, p2_tsne2, df2_tsne2 = ttest_ind(group_1_4_tsne2, group_9_12_tsne2)\n",
    "    t3_tsne2, p3_tsne2, df3_tsne2 = ttest_ind(group_5_8_tsne2, group_9_12_tsne2)\n",
    "\n",
    "    print(f\"Sample size: {sample_size}, Max features: {max_features}\")\n",
    "    print(\"T-test results for tsne1:\")\n",
    "    print(\n",
    "        \"T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f\"\n",
    "        % (t1_tsne1, p1_tsne1)\n",
    "    )\n",
    "    print(\n",
    "        \"T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f\"\n",
    "        % (t2_tsne1, p2_tsne1)\n",
    "    )\n",
    "    print(\n",
    "        \"T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f\"\n",
    "        % (t3_tsne1, p3_tsne1)\n",
    "    )\n",
    "    print(\"T-test results for tsne1:\")\n",
    "    print(\n",
    "        \"T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f\"\n",
    "        % (t1_tsne2, p1_tsne2)\n",
    "    )\n",
    "    print(\n",
    "        \"T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f\"\n",
    "        % (t2_tsne2, p2_tsne2)\n",
    "    )\n",
    "    print(\n",
    "        \"T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f\"\n",
    "        % (t3_tsne2, p3_tsne2)\n",
    "    )\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
