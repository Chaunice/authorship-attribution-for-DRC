{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Attribution for *Dream of the Red Chamber* (红楼梦)\n",
    "\n",
    "- author: 'chaunice@163.com'\n",
    "- date: '2024-01-26'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "with reference to [Prose, Verse and Authorship in Dream of the Red Chamber: A Stylometric Analysis](https://www.researchgate.net/profile/Haoran-Zhu-8/publication/339140299_Prose_Verse_and_Authorship_in_Dream_of_the_Red_Chamber_A_Stylometric_Analysis/links/650fccd661f18040c21a3ff9/Prose-Verse-and-Authorship-in-Dream-of-the-Red-Chamber-A-Stylometric-Analysis.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "- DRC text from https://github.com/buobao/HLM-txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from statsmodels.stats.weightstats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_verses_and_prose(data):\n",
    "    # Compile regex patterns\n",
    "    verse_pattern = re.compile(r'<blockquote([\\s\\S]*?)</blockquote>')\n",
    "    prose_pattern = re.compile(r'<p>([\\s\\S]*?)</p>')\n",
    "\n",
    "    # Find all verses\n",
    "    verses = re.findall(verse_pattern, data)\n",
    "    verses = [verse.strip() for verse in verses]  # Remove leading/trailing whitespace\n",
    "\n",
    "    # Remove verses from data\n",
    "    data = re.sub(verse_pattern, '', data)\n",
    "\n",
    "    # Find all prose\n",
    "    prose = re.findall(prose_pattern, data)\n",
    "    prose = [p.strip() for p in prose]  # Remove leading/trailing whitespace\n",
    "\n",
    "    return verses, prose\n",
    "\n",
    "def glob_markdown_files(directory):\n",
    "    return list(Path(directory).glob('*.md'))\n",
    "\n",
    "# batch process all files and store results in a csv while keeping track of the file name\n",
    "def batch_process_files(directory):\n",
    "    markdown_files = glob_markdown_files(directory)\n",
    "    results = []\n",
    "    for file in markdown_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = f.read()\n",
    "        verses, prose = separate_verses_and_prose(data)\n",
    "        results.append({'File': file.name, 'Verses': verses, 'Prose': prose})\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('DRC_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_process_files('full/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DRC_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the text by removing punctuations and only retaining chinese characters\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fff]', '', text)\n",
    "    return text\n",
    "\n",
    "df['Verses'] = df['Verses'].apply(lambda x: [clean_text(verse) for verse in x])\n",
    "df['Prose'] = df['Prose'].apply(lambda x: [clean_text(p) for p in x])\n",
    "\n",
    "# remove empty verses and prose\n",
    "df['Verses'] = df['Verses'].apply(lambda x: [verse for verse in x if verse])\n",
    "df['Prose'] = df['Prose'].apply(lambda x: [p for p in x if p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('DRC_text_clean.csv', index=False, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DRC_text_clean.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of characters in each verse and prose group by 10 files so that we get 12 groups\n",
    "df['VerseLength'] = df['Verses'].apply(lambda x: [len(verse) for verse in x])\n",
    "df['ProseLength'] = df['Prose'].apply(lambda x: [len(p) for p in x])\n",
    "df['VerseLength'] = df['VerseLength'].apply(lambda x: [len(x)])\n",
    "df['ProseLength'] = df['ProseLength'].apply(lambda x: [len(x)])\n",
    "\n",
    "# 10 files as a group\n",
    "df['Groups'] = (df.index // 10) + 1\n",
    "\n",
    "# sum the length of verses and prose in each group\n",
    "df = df.groupby('Groups').sum()\n",
    "# calculate the sum of verses and prose in each group\n",
    "df['VerseCount'] = df['VerseLength'].apply(lambda x: sum(x))\n",
    "df['ProseCount'] = df['ProseLength'].apply(lambda x: sum(x))\n",
    "\n",
    "df['Proportion of Verse'] = df['VerseCount'] / (df['VerseCount'] + df['ProseCount'])\n",
    "\n",
    "df['Groups'] = df.index\n",
    "\n",
    "# 用正则表达式删除所有标点符号和空格\n",
    "df['Verses'] = df['Verses'].apply(lambda x: re.sub(r'[^\\u4e00-\\u9fff]', '', x))\n",
    "df['Prose'] = df['Prose'].apply(lambda x: re.sub(r'[^\\u4e00-\\u9fff]', '', x))\n",
    "\n",
    "# 创建一个新的列来区分前4组，中间4组和后4组\n",
    "df['Group_sep'] = ['Group 1-4']*4 + ['Group 5-8']*4 + ['Group 9-12']*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results concentrating 'Proportion of Verse'\n",
    "# 创建一个字典，将组名映射到 Chicago 配色的颜色代码\n",
    "chicago_palette = {'Group 1-4': '#0b6623', 'Group 5-8': '#0F425CFF', 'Group 9-12': '#a40e4c'}\n",
    "\n",
    "# 创建柱状图\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x=df.index, y=df['Proportion of Verse'], hue=df['Group_sep'], dodge=False, palette=chicago_palette)\n",
    "\n",
    "# 计算前8组和后4组的平均值\n",
    "average_line_1_4 = df.loc[1:4, 'Proportion of Verse'].mean()\n",
    "average_line_5_8 = df.loc[5:8, 'Proportion of Verse'].mean()\n",
    "average_line_9_12 = df.loc[9:12, 'Proportion of Verse'].mean()\n",
    "\n",
    "# 绘制平均线\n",
    "plt.axhline(average_line_1_4, color='#0b6623', linestyle='--', label='Average Line (Group 1-8)')\n",
    "plt.axhline(average_line_5_8, color='#0F425CFF', linestyle='--', label='Average Line (Group 5-8)')\n",
    "plt.axhline(average_line_9_12, color='#a40e4c', linestyle='--', label='Average Line (Group 9-12)')\n",
    "\n",
    "plt.xlabel('Groups')\n",
    "plt.ylabel('Proportion of Verse')\n",
    "plt.title('Proportion of Verse in Each Group')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform T-test to see if there is a significant difference between group 1-4, group 5-8 and group 9-12\n",
    "ttest_df = df[['Proportion of Verse', 'Group_sep']]\n",
    "\n",
    "# 将数据分成三组\n",
    "group_1_4 = ttest_df[ttest_df['Group_sep'] == 'Group 1-4']['Proportion of Verse']\n",
    "group_5_8 = ttest_df[ttest_df['Group_sep'] == 'Group 5-8']['Proportion of Verse']\n",
    "group_9_12 = ttest_df[ttest_df['Group_sep'] == 'Group 9-12']['Proportion of Verse']\n",
    "\n",
    "# 进行 T 检验\n",
    "t1, p1, df1 = ttest_ind(group_1_4, group_5_8)\n",
    "t2, p2, df2 = ttest_ind(group_1_4, group_9_12)\n",
    "t3, p3, df3 = ttest_ind(group_5_8, group_9_12)\n",
    "\n",
    "print('T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f' % (t1, p1))\n",
    "print('T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f' % (t2, p2))\n",
    "print('T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f' % (t3, p3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，从一个字符串中随机抽取n个字符\n",
    "def sample_chars(s, n):\n",
    "    return ''.join(random.sample(s, min(n, len(s))))\n",
    "\n",
    "def cmex_pca(dataframe, sample_size, max_features, n_components):\n",
    "    # 创建 df 的副本，因为我们将修改它\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # 重复100次，每次都从每个组中随机抽取5000个字符，然后计算字符计数矩阵\n",
    "    char_counts_list = []\n",
    "    for i in range(100):\n",
    "        df_sample = df.groupby('Group_sep').apply(lambda x: sample_chars(''.join(x['Prose']), sample_size), include_groups=False).reset_index()\n",
    "        df_sample[0] = df_sample[0].apply(lambda x: ''.join(list(x)))\n",
    "        vectorizer = CountVectorizer(analyzer='char', max_features=max_features)\n",
    "        char_counts = vectorizer.fit_transform(df_sample[0])\n",
    "        char_counts_df = pd.DataFrame(char_counts.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "        char_counts_list.append(char_counts_df)\n",
    "        \n",
    "    # 将所有字符计数矩阵连接起来，保留分组信息\n",
    "    char_counts_df = pd.concat(char_counts_list, keys=range(100))\n",
    "\n",
    "    # 将空值替换为 0\n",
    "    char_counts_df = char_counts_df.fillna(0)\n",
    "\n",
    "    # 转换 char_counts_df，将分组信息转换为列\n",
    "    char_counts_df = char_counts_df.reset_index().rename(columns={'level_0': 'Sample', 'level_1': 'Group_sep'})\n",
    "\n",
    "    # 分组进行PCA，保留分组信息方便后续绘图\n",
    "    pca = PCA(n_components=n_components)\n",
    "    char_counts_df_pca = pca.fit_transform(char_counts_df.drop(['Sample', 'Group_sep'], axis=1))\n",
    "    char_counts_df_pca = pd.DataFrame(char_counts_df_pca, columns=['PC1', 'PC2'])\n",
    "    char_counts_df_pca['Sample'] = char_counts_df['Sample']\n",
    "    char_counts_df_pca['Group_sep'] = char_counts_df['Group_sep']\n",
    "    return char_counts_df_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个空字典来保存结果\n",
    "pca_results = {}\n",
    "\n",
    "# 遍历样本大小\n",
    "for sample_size in [1000, 2000, 2500, 3000, 5000]:\n",
    "    # 遍历特征数量\n",
    "    for max_features in [25, 50, 75, 100, 150]:\n",
    "        # 调用函数并保存结果\n",
    "        key = (sample_size, max_features)\n",
    "        pca_results[key] = cmex_pca(df, sample_size, max_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 5x5 的图像网格\n",
    "fig, axs = plt.subplots(5, 5, figsize=(25, 25))\n",
    "\n",
    "# 遍历所有的结果\n",
    "for i, ((sample_size, max_features), temp_df_pca) in enumerate(pca_results.items()):\n",
    "    # 计算当前子图的位置\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "\n",
    "    # 在当前子图中绘制散点图\n",
    "    ax = axs[row, col]\n",
    "    chicago_palette = {0: '#C16622FF', 1: '#155F83FF', 2: '#3E3E23FF'}\n",
    "    sns.scatterplot(data=temp_df_pca, x='PC1', y='PC2', hue='Group_sep', palette=chicago_palette, ax=ax)\n",
    "\n",
    "    # 设置子图的标题\n",
    "    ax.set_title(f'Sample size: {sample_size}, Max features: {max_features}')\n",
    "\n",
    "# 调整子图之间的间距\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test on PCA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ((sample_size, max_features), temp_df_pca_ttest) in enumerate(pca_results.items()):\n",
    "    \n",
    "    pca_result_ttest_df = temp_df_pca_ttest[['PC1', 'PC2', 'Group_sep']]\n",
    "\n",
    "    # transform the dataframe to a long format\n",
    "    pca_result_ttest_df = pca_result_ttest_df.melt(id_vars=['Group_sep'], value_vars=['PC1', 'PC2'])\n",
    "\n",
    "    # retain Group_sep, meanwhile transform PC1 and PC2 to separate columns, turn the values to a list\n",
    "    pca_result_ttest_df = pca_result_ttest_df.groupby(['Group_sep', 'variable'])['value'].apply(list).reset_index()\n",
    "    \n",
    "    # perform T-test to see if there is a significant difference between group 1-4, group 5-8 and group 9-12\n",
    "    ttest_df = pca_result_ttest_df[['value', 'Group_sep', 'variable']]\n",
    "    ttest_df = ttest_df.explode('value')\n",
    "\n",
    "    # 将数据分成三组\n",
    "    group_1_4_PC1 = ttest_df.query('variable == \"PC1\" and Group_sep == 0')['value']\n",
    "    group_5_8_PC1 = ttest_df.query('variable == \"PC1\" and Group_sep == 1')['value']\n",
    "    group_9_12_PC1 = ttest_df.query('variable == \"PC1\" and Group_sep == 2')['value']\n",
    "    \n",
    "    group_1_4_PC2 = ttest_df.query('variable == \"PC2\" and Group_sep == 0')['value']\n",
    "    group_5_8_PC2 = ttest_df.query('variable == \"PC2\" and Group_sep == 1')['value']\n",
    "    group_9_12_PC2 = ttest_df.query('variable == \"PC2\" and Group_sep == 2')['value']\n",
    "\n",
    "    # 进行 T 检验\n",
    "    t1_PC1, p1_PC1, df1_PC1 = ttest_ind(group_1_4_PC1, group_5_8_PC1)\n",
    "    t2_PC1, p2_PC1, df2_PC1 = ttest_ind(group_1_4_PC1, group_9_12_PC1)\n",
    "    t3_PC1, p3_PC1, df3_PC1 = ttest_ind(group_5_8_PC1, group_9_12_PC1)\n",
    "    \n",
    "    t1_PC2, p1_PC2, df1_PC2 = ttest_ind(group_1_4_PC2, group_5_8_PC2)\n",
    "    t2_PC2, p2_PC2, df2_PC2 = ttest_ind(group_1_4_PC2, group_9_12_PC2)\n",
    "    t3_PC2, p3_PC2, df3_PC2 = ttest_ind(group_5_8_PC2, group_9_12_PC2)\n",
    "    \n",
    "    print(f'Sample size: {sample_size}, Max features: {max_features}')\n",
    "    print('T-test results for PC1:')\n",
    "    print('T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f' % (t1_PC1, p1_PC1))\n",
    "    print('T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f' % (t2_PC1, p2_PC1))\n",
    "    print('T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f' % (t3_PC1, p3_PC1))\n",
    "    print('T-test results for PC2:')\n",
    "    print('T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f' % (t1_PC2, p1_PC2))\n",
    "    print('T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f' % (t2_PC2, p2_PC2))\n",
    "    print('T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f' % (t3_PC2, p3_PC2))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE: t-distributed Stochastic Neighbor Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform t-sne to see if there is a significant difference between group 1-4, group 5-8 and group 9-12\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def cmex_tsne(dataframe, sample_size, max_features, n_components):\n",
    "    # 创建 df 的副本，因为我们将修改它\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # 重复100次，每次都从每个组中随机抽取5000个字符，然后计算字符计数矩阵\n",
    "    char_counts_list = []\n",
    "    for i in range(100):\n",
    "        df_sample = df.groupby('Group_sep').apply(lambda x: sample_chars(''.join(x['Prose']), sample_size), include_groups=False).reset_index()\n",
    "        df_sample[0] = df_sample[0].apply(lambda x: ''.join(list(x)))\n",
    "        vectorizer = CountVectorizer(analyzer='char', max_features=max_features)\n",
    "        char_counts = vectorizer.fit_transform(df_sample[0])\n",
    "        char_counts_df = pd.DataFrame(char_counts.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "        char_counts_list.append(char_counts_df)\n",
    "        \n",
    "    # 将所有字符计数矩阵连接起来，保留分组信息\n",
    "    char_counts_df = pd.concat(char_counts_list, keys=range(100))\n",
    "\n",
    "    # 将空值替换为 0\n",
    "    char_counts_df = char_counts_df.fillna(0)\n",
    "\n",
    "    # 转换 char_counts_df，将分组信息转换为列\n",
    "    char_counts_df = char_counts_df.reset_index().rename(columns={'level_0': 'Sample', 'level_1': 'Group_sep'})\n",
    "\n",
    "    # 分组进行t-sne，保留分组信息方便后续绘图\n",
    "    tsne = TSNE(n_components=n_components)\n",
    "    char_counts_df_tsne = tsne.fit_transform(char_counts_df.drop(['Sample', 'Group_sep'], axis=1))\n",
    "    char_counts_df_tsne = pd.DataFrame(char_counts_df_tsne, columns=['tsne1', 'tsne2'])\n",
    "    char_counts_df_tsne['Sample'] = char_counts_df['Sample']\n",
    "    char_counts_df_tsne['Group_sep'] = char_counts_df['Group_sep']\n",
    "    return char_counts_df_tsne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个空字典来保存结果\n",
    "tsne_results = {}\n",
    "\n",
    "# 遍历样本大小\n",
    "for sample_size in [1000, 2000, 2500, 3000, 5000]:\n",
    "    # 遍历特征数量\n",
    "    for max_features in [25, 50, 75, 100, 150]:\n",
    "        # 调用函数并保存结果\n",
    "        key = (sample_size, max_features)\n",
    "        tsne_results[key] = cmex_tsne(df, sample_size, max_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 5x5 的图像网格\n",
    "fig, axs = plt.subplots(5, 5, figsize=(25, 25))\n",
    "\n",
    "# 遍历所有的结果\n",
    "for i, ((sample_size, max_features), temp_df_tsne) in enumerate(tsne_results.items()):\n",
    "    # 计算当前子图的位置\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "\n",
    "    # 在当前子图中绘制散点图\n",
    "    ax = axs[row, col]\n",
    "    chicago_palette = {0: '#C16622FF', 1: '#155F83FF', 2: '#3E3E23FF'}\n",
    "    sns.scatterplot(data=temp_df_tsne, x='tsne1', y='tsne2', hue='Group_sep', palette=chicago_palette, ax=ax)\n",
    "\n",
    "    # 设置子图的标题\n",
    "    ax.set_title(f'Sample size: {sample_size}, Max features: {max_features}')\n",
    "\n",
    "# 调整子图之间的间距\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test on T-SNE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ((sample_size, max_features), temp_df_tsne_ttest) in enumerate(tsne_results.items()):\n",
    "    \n",
    "    tsne_result_ttest_df = temp_df_tsne_ttest[['tsne1', 'tsne2', 'Group_sep']]\n",
    "\n",
    "    # transform the dataframe to a long format\n",
    "    tsne_result_ttest_df = tsne_result_ttest_df.melt(id_vars=['Group_sep'], value_vars=['tsne1', 'tsne2'])\n",
    "\n",
    "    # retain Group_sep, meanwhile transform PC1 and PC2 to separate columns, turn the values to a list\n",
    "    tsne_result_ttest_df = tsne_result_ttest_df.groupby(['Group_sep', 'variable'])['value'].apply(list).reset_index()\n",
    "    \n",
    "    # perform T-test to see if there is a significant difference between group 1-4, group 5-8 and group 9-12\n",
    "    ttest_df = tsne_result_ttest_df[['value', 'Group_sep', 'variable']]\n",
    "    ttest_df = ttest_df.explode('value')\n",
    "\n",
    "    # 将数据分成三组\n",
    "    group_1_4_tsne1 = ttest_df.query('variable == \"tsne1\" and Group_sep == 0')['value']\n",
    "    group_5_8_tsne1 = ttest_df.query('variable == \"tsne1\" and Group_sep == 1')['value']\n",
    "    group_9_12_tsne1 = ttest_df.query('variable == \"tsne1\" and Group_sep == 2')['value']\n",
    "    \n",
    "    group_1_4_tsne2 = ttest_df.query('variable == \"tsne2\" and Group_sep == 0')['value']\n",
    "    group_5_8_tsne2 = ttest_df.query('variable == \"tsne2\" and Group_sep == 1')['value']\n",
    "    group_9_12_tsne2 = ttest_df.query('variable == \"tsne2\" and Group_sep == 2')['value']\n",
    "\n",
    "    # 进行 T 检验\n",
    "    t1_tsne1, p1_tsne1, df1_tsne1 = ttest_ind(group_1_4_tsne1, group_5_8_tsne1)\n",
    "    t2_tsne1, p2_tsne1, df2_tsne1 = ttest_ind(group_1_4_tsne1, group_9_12_tsne1)\n",
    "    t3_tsne1, p3_tsne1, df3_tsne1 = ttest_ind(group_5_8_tsne1, group_9_12_tsne1)\n",
    "    \n",
    "    t1_tsne2, p1_tsne2, df1_tsne2 = ttest_ind(group_1_4_tsne2, group_5_8_tsne2)\n",
    "    t2_tsne2, p2_tsne2, df2_tsne2 = ttest_ind(group_1_4_tsne2, group_9_12_tsne2)\n",
    "    t3_tsne2, p3_tsne2, df3_tsne2 = ttest_ind(group_5_8_tsne2, group_9_12_tsne2)\n",
    "    \n",
    "    print(f'Sample size: {sample_size}, Max features: {max_features}')\n",
    "    print('T-test results for tsne1:')\n",
    "    print('T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f' % (t1_tsne1, p1_tsne1))\n",
    "    print('T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f' % (t2_tsne1, p2_tsne1))\n",
    "    print('T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f' % (t3_tsne1, p3_tsne1))\n",
    "    print('T-test results for tsne1:')\n",
    "    print('T-test results for Group 1-4 and Group 5-8: t = %.3f, p = %.3f' % (t1_tsne2, p1_tsne2))\n",
    "    print('T-test results for Group 1-4 and Group 9-12: t = %.3f, p = %.3f' % (t2_tsne2, p2_tsne2))\n",
    "    print('T-test results for Group 5-8 and Group 9-12: t = %.3f, p = %.3f' % (t3_tsne2, p3_tsne2))\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
